{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0c03a2-0c5a-481c-9c1a-ed15ba5c1b91",
   "metadata": {},
   "source": [
    "Data Mining - Mid Term Project(sm3677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1cf69-f68e-48d1-bea4-454e55ea7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "import timeit\n",
    "\n",
    "# Function to generate potential itemsets\n",
    "def generate_itemsets(items):\n",
    "    subsets = [[]]\n",
    "    for item in items:\n",
    "        new_combinations = [subset + [item] for subset in subsets]\n",
    "        subsets.extend(new_combinations)\n",
    "    return subsets[1:]\n",
    "\n",
    "# Brute Force Approach for Association Rule Mining\n",
    "def brute_force_association(dataframe, min_support, min_confidence):\n",
    "    transactions = dataframe[\"Items\"].str.split(\", \")\n",
    "    num_transactions = len(transactions)\n",
    "    \n",
    "    item_list = [item for sublist in transactions for item in sublist]\n",
    "    item_counts = pd.Series(item_list).value_counts()\n",
    "    \n",
    "    # Filtering based on minimum support\n",
    "    freq_items = item_counts[item_counts >= (min_support / 100) * num_transactions].reset_index()\n",
    "    freq_items.columns = [\"Itemsets\", \"Frequency\"]\n",
    "    freq_items[\"Support\"] = (freq_items[\"Frequency\"] / num_transactions) * 100\n",
    "    \n",
    "    # Generate possible itemsets\n",
    "    potential_itemsets = generate_itemsets(freq_items[\"Itemsets\"])\n",
    "    valid_itemsets = [iset for iset in potential_itemsets if len(iset) > 1]\n",
    "    \n",
    "    # Count occurrences of each itemset\n",
    "    itemset_occurrences = {}\n",
    "    for i_set in valid_itemsets:\n",
    "        count = sum(1 for transaction in transactions if set(i_set).issubset(set(transaction)))\n",
    "        itemset_occurrences[tuple(i_set)] = count\n",
    "    \n",
    "    itemset_df = pd.DataFrame(list(itemset_occurrences.items()), columns=[\"Itemsets\", \"Frequency\"])\n",
    "    itemset_df[\"Support\"] = (itemset_df[\"Frequency\"] / num_transactions) * 100\n",
    "    \n",
    "    # Combine frequent single items with frequent itemsets\n",
    "    all_frequent_items = pd.concat([freq_items, itemset_df[itemset_df[\"Support\"] >= min_support]], ignore_index=True)\n",
    "    \n",
    "    # Generate association rules based on confidence threshold\n",
    "    item_support_map = dict(zip(all_frequent_items[\"Itemsets\"], all_frequent_items[\"Support\"]))\n",
    "    association_dict = {}\n",
    "    support_values = []\n",
    "    \n",
    "    for antecedent in item_support_map:\n",
    "        for consequent in item_support_map:\n",
    "            if set(consequent).issubset(set(antecedent)) and antecedent != consequent:\n",
    "                conf_value = (item_support_map[antecedent] / item_support_map[consequent]) * 100\n",
    "                association_dict[(antecedent, consequent)] = conf_value\n",
    "                support_values.append(item_support_map[antecedent])\n",
    "    \n",
    "    rule_df = pd.DataFrame(association_dict.items(), columns=[\"Rules\", \"Confidence\"])\n",
    "    rule_df[\"Support\"] = support_values\n",
    "    rule_df = rule_df[rule_df[\"Confidence\"] >= min_confidence].reset_index(drop=True)\n",
    "    \n",
    "    return rule_df, all_frequent_items\n",
    "\n",
    "# Apriori Algorithm\n",
    "\n",
    "def apriori_algorithm(dataframe, min_support, min_confidence):\n",
    "    transaction_matrix = dataframe[\"Items\"].str.get_dummies(sep=\", \").astype(bool)\n",
    "    frequent_itemsets = apriori(transaction_matrix, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    frequent_itemsets.rename(columns={\"itemsets\": \"Itemsets\", \"support\": \"Support\"}, inplace=True)\n",
    "    frequent_itemsets[\"Support\"] *= 100\n",
    "    return rules, frequent_itemsets\n",
    "\n",
    "# FP-Growth Algorithm\n",
    "def fpgrowth_algorithm(dataframe, min_support, min_confidence):\n",
    "    transaction_matrix = dataframe[\"Items\"].str.get_dummies(sep=\", \").astype(bool)\n",
    "    frequent_itemsets = fpgrowth(transaction_matrix, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    frequent_itemsets.rename(columns={\"itemsets\": \"Itemsets\", \"support\": \"Support\"}, inplace=True)\n",
    "    frequent_itemsets[\"Support\"] *= 100\n",
    "    return rules, frequent_itemsets\n",
    "\n",
    "# Display results for Brute Force Approach\n",
    "def display_manual_rules(rule_df):\n",
    "    for idx, row in rule_df.iterrows():\n",
    "        print(f\"Rule_{idx + 1}: {row['Rules'][0]} --> {row['Rules'][1]} Support: {row['Support']} Confidence: {row['Confidence']}\\n\")\n",
    "\n",
    "# Display results for Apriori and FP-Growth Approaches\n",
    "def display_algorithmic_rules(rule_df):\n",
    "    for idx, row in rule_df.iterrows():\n",
    "        print(f\"Rule_{idx + 1}: {tuple(row['antecedents'])} --> {tuple(row['consequents'])} Support: {row['support'] * 100} Confidence: {row['confidence'] * 100}\\n\")\n",
    "\n",
    "# Company Selection\n",
    "companies = {1: \"amazon\", 2: \"costco\", 3: \"ikea\", 4: \"target\", 5: \"7-11\", 6: \"walmart\", 7: \"Quit\"}\n",
    "\n",
    "while True:\n",
    "    print(\"\\nWelcome to Market Basket Analysis Tool\\n\")\n",
    "    for num, name in companies.items():\n",
    "        print(f\"{num}: {name}\")\n",
    "    \n",
    "    choice = int(input(\"\\nSelect a company from the above list: \"))\n",
    "    if choice == 7:\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nSelected: {companies.get(choice, 'Invalid Choice')}\")\n",
    "    min_sup = float(input(\"Enter minimum support (1-100%): \"))\n",
    "    min_conf = float(input(\"Enter minimum confidence (1-100%): \"))\n",
    "    \n",
    "    if choice in companies:\n",
    "        data_url = f\"https://raw.githubusercontent.com/Pikaboo69/Datasets/refs/heads/main/{companies[choice]}.csv\"\n",
    "        transaction_data = pd.read_csv(data_url)\n",
    "        \n",
    "        # Brute Force Execution\n",
    "        bf_rules, bf_freq_items = brute_force_association(transaction_data, min_sup, min_conf)\n",
    "        print(\"\\nBrute Force Frequent Itemsets:\\n\", bf_freq_items)\n",
    "        print(\"\\nBrute Force Association Rules:\")\n",
    "        display_manual_rules(bf_rules)\n",
    "        \n",
    "        # Apriori Execution\n",
    "        apriori_rules, apriori_items = apriori_algorithm(transaction_data, min_sup / 100, min_conf / 100)\n",
    "        print(\"\\nApriori Frequent Itemsets:\\n\", apriori_items)\n",
    "        print(\"\\nApriori Association Rules:\")\n",
    "        display_algorithmic_rules(apriori_rules)\n",
    "        \n",
    "        # FP-Growth Execution\n",
    "        fp_rules, fp_items = fpgrowth_algorithm(transaction_data, min_sup / 100, min_conf / 100)\n",
    "        print(\"\\nFP-Growth Frequent Itemsets:\\n\", fp_items)\n",
    "        print(\"\\nFP-Growth Association Rules:\")\n",
    "        display_algorithmic_rules(fp_rules)\n",
    "        \n",
    "        # Execution Time Comparison\n",
    "        print(f\"Execution Time (Brute Force): {timeit.timeit(lambda: brute_force_association(transaction_data, min_sup, min_conf), number=1)} sec\")\n",
    "        print(f\"Execution Time (Apriori): {timeit.timeit(lambda: apriori_algorithm(transaction_data, min_sup / 100, min_conf / 100), number=1)} sec\")\n",
    "        print(f\"Execution Time (FP-Growth): {timeit.timeit(lambda: fpgrowth_algorithm(transaction_data, min_sup / 100, min_conf / 100), number=1)} sec\")\n",
    "    else:\n",
    "        print(\"Invalid choice, please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e945a-7d94-4e0e-9f90-8a096d293e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563d6fe-2248-4794-980a-0fa9fc6cf063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
